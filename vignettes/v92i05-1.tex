\documentclass[article]{jss}
\usepackage[crop=off]{auto-pst-pdf}
\usepackage{amsmath,amssymb,bbm,pstricks,pst-tree,array,subcaption,threeparttable,thumbpdf,lmodern} 
\graphicspath{{Figures/}}

\newcommand{\qq}[1]{``{#1}''}
\newcommand{\fct}[1]{\code{#1()}}
\newcommand{\class}[1]{`\code{#1}'}

\makeatletter
\let\old@code\code
\renewcommand{\code}[1]{%
  \ifmmode\text{\old@code{#1}}%
  \else\old@code{#1}\fi}
\makeatother

\newcommand{\SSS}{\proglang{S}3}
\newcommand{\R}{\proglang{R}}
\newcommand{\CXX}{\proglang{C++}}

\newcommand{\drop}{\mathrm{drop}}
\newcommand{\Rset}{\mathbb{R}}
\newcommand{\rss}{\mathrm{RSS}}
\newcommand{\card}[1]{\left\lvert{#1}\right\rvert}
\newcommand{\norm}[1]{\left\lVert{#1}\right\lVert_2}
\DeclareMathOperator*{\argmin}{argmin}


\author{
  Marc Hofmann\\University of Oviedo\\Cyprus University of Technology
  \And Cristian Gatu\\``Alexandru Ioan Cuza''\\University of Iasi
  \And Erricos J. Kontoghiorghes\\Cyprus University of Technology\\Birkbeck, University of London
  \AND Ana Colubi\\University of Oviedo\\Justus-Liebig-University Giessen
  \And Achim Zeileis\\Universit\"at Innsbruck
}
\Plainauthor{Marc Hofmann, Cristian Gatu, Erricos J. Kontoghiorghes, Ana Colubi, Achim Zeileis}

\title{\pkg{lmSubsets}: Exact Variable-Subset Selection in Linear Regression for \proglang{R}}
\Plaintitle{lmSubsets: Exact Variable-Subset Selection in Linear Regression for R}

\Abstract{
  An {\R} package for computing the all-subsets regression problem is
  presented.  The proposed algorithms are based on computational
  strategies recently developed.  A novel algorithm for the
  best-subset regression problem selects subset models based on a
  pre-determined criterion.  The package user can choose from exact
  and from approximation algorithms.  The core of the package is
  written in {\CXX} and provides an efficient implementation of all
  the underlying numerical computations.  A case study and benchmark
  results illustrate the usage and the computational efficiency of the
  package.
}

\Keywords{linear regression, model selection, variable selection, best-subset regression, \proglang{R}}
\Plainkeywords{linear regression, model selection, variable selection, best-subset regression, R}

\Volume{92}
\Issue{5}
\Month{February}
\Year{2020}
\Submitdate{2018-05-02}
\Acceptdate{2018-11-11}
\DOI{10.18637/jss.v092.i05}

\Address{
  Marc Hofmann\\
  Corresponding author\\
  Institute of Natural Resources and Territorial Planning\\
  University of Oviedo\\
  33600 Mieres, Spain\\
  E-mail: \email{marc.hofmann@gmail.com}, \email{marc.indurot@uniovi.es}\\
  URL: \url{http://www.indurot.uniovi.es/}
}

\begin{document}

\vspace*{-0.5cm}

\section{Introduction}
\label{sec:intro}

An important problem in statistical modeling is that of subset
selection regression or, equivalently, of finding the best regression
equation~\citep{clarke:j_roy_stat_soc_c_app:81,hastie:01}.  Given a
set of possible variables to be included in the regression, the
problem consists in selecting a subset that optimizes some statistical
criterion.  The evaluation of the criterion function typically
involves the estimation of the corresponding
submodel~\citep{miller:02}.  Consider the standard regression model
%
\begin{equation}
  \label{eq:olm}
  %
  y=X\beta+\epsilon\text{ ,}
\end{equation}
%
where $y\in\Rset^M$ is the output variable, $X\in\Rset^{M\times N}$ is
the regressor matrix of full column rank, $\beta\in\Rset^N$ is the
coefficient vector, and $\epsilon\in\Rset^M$ is the noise vector.  The
ordinary least squares (OLS) estimator of $\beta$ is the solution of
%
\begin{equation}
  \hat{\beta}_{\text{OLS}}=\argmin_{\beta}\rss(\beta)\text{ ,}
\end{equation}
%
where the residual sum of squares (RSS) of $\beta$ is given by
%
\begin{equation}
  \rss(\beta)=\norm{y-X\beta}^2\text{ .}
\end{equation}
%
That is, $\hat{\beta}_{\text{OLS}}$ minimizes the norm of the residual
vector.  The regression coefficients $\beta$ do not need to be
explicitly computed in order to determine the RSS, which can be
obtained through numerically stable orthogonal matrix decomposition
methods~\citep{golub:96}.
  
Let $V=\{1,\ldots,N\}$ denote the set of all independent variables.  A
subset model (or submodel) is denoted by $S$, $S\subseteq V$.  Given a
criterion function $f$, the best-subset selection problem consists in
solving
%
\begin{equation}
  \label{eq:best_subset}
  %
  S^*=\argmin_{S\subseteq V}f(S)\text{ .}
\end{equation}
%
Here, the value $f(S)=F(n,\rho)$ is seen as a function of $n=\card{S}$
and $\rho=\rss(S)$, the number of selected variables and the RSS of
the OLS estimator for $S$, respectively.  Furthermore, it is assumed
that $f(S)$ is monotonic with respect to $\rss(S)$ for fixed $n$, that
is
%
\begin{equation}
  \label{eq:f:monotonicity}
  %
  \rss(S_1)\leq\rss(S_2)\implies f(S_1)\leq f(S_2)\text{ ,}
  \quad\text{when}\quad
  \quad\card{S_1}=\card{S_2}\text{ .}
\end{equation}

Common information criteria (IC) exhibit this property, such as those
belonging to the AIC family and defined by the formula
%
\begin{equation}
  \label{eq:aic}
  %
  \text{AIC}_k=M+M\log2\pi+M\log(\rss/M)+k(n+1)\text{ ,}
\end{equation}
%
where the scalar $k$ represents a penalty per parameter ($k>0$).  The
usual AIC and BIC are obtained for $k=2$ and $k=\log M$,
respectively~\citep{miller:02}.  It follows
that~\eqref{eq:best_subset} is equivalent to
%
\begin{equation*}
  S^*=S^*_\nu\text{ ,}
  \quad\text{where}\quad
  \nu=\argmin_{n}f(S^*_n)
\end{equation*}
%
and
%
\begin{equation}
  \label{eq:all_subsets}
  %
  S^*_n=\argmin_{\card{S}=n}\rss(S)
  \quad\text{for}\quad
  n=1,\dots,N\text{ .}
\end{equation}
%
Finding the solution to~\eqref{eq:all_subsets} is called the
all-subsets selection problem.  Thus, solving~\eqref{eq:best_subset}
can be seen as an indirect, two-stage procedure:
%
\begin{description}
\item[Stage 1] For each size $n$, find the subset $S^*_n$
  ($\card{S^*_n}=n$) with the smallest RSS.
\item[Stage 2] Compute $f(S^*_n)$ for all $n$, and determine $\nu$
  such that $f(S^*_\nu)$ is minimal.
\end{description}
%
By explicitly solving the all-subsets regression
problem~\eqref{eq:all_subsets} once and for all (Stage 1), the list of
all $N$ submodels is made readily available for further exploration:
Evaluating multiple criterion functions (e.g., AIC and BIC), or
conducting a more elaborate statistical inference, can be performed at
a negligible cost (Stage 2).  Thus, it may be advisable to adopt a
two-stage approach within the scope of a broader and more thorough
statistical investigation.  On the other hand, precursory knowledge of
the search function and of its characteristics opens up the
possibility for a custom-tailored computational strategy to solve the
best-subset selection problem~\eqref{eq:best_subset} in one go; by
exploiting more information about the problem at hand, the solution
strategy will be rendered more efficient.

Brute-force (or exhaustive) search procedures that enumerate all
possible subsets are often intractable even for a modest number of
variables.  Exact algorithms must employ techniques to reduce the size
of the search space -- i.e., the number of enumerated subsets -- in
order to tackle larger problems.  Heuristic algorithms renounce
optimality in order to decrease execution times: They are designed for
solving a problem more quickly, but make no guarantees on the quality
of the solution produced; genetic algorithms and simulated annealing
are among the well-known heuristic
algorithms~\citep{goldberg:89,otten:89}.  The solution returned by an
approximation algorithm, on the other hand, can be proven to lie
within well specified bounds of the optimum.

Several packages that deal with variable subset selection are
available for the {\R} environment for statistical computing and
graphics \citep{R}.  Package \pkg{leaps}~\citep{leaps} implements
exact, exhaustive and non-exhaustive algorithms for subset selection
in linear models~\citep{miller:02}; it has been extended to
generalized linear models by package \pkg{bestglm}~\citep{bestglm}.
An active set algorithm for solving the best subset selection problem
in generalized linear models is proposed by package
\pkg{BeSS}~\citep{BeSS}.  Package \pkg{subselect}~\citep{subselect}
proposes simulated annealing and genetic algorithms that search for
subsets of variables which are optimal under various criteria.
Package \pkg{glmulti}~\citep{glmulti} provides IC-based automated
model selection methods for generalized linear models in the form of
exhaustive and genetic algorithms.  Package
\pkg{kofnGA}~\citep{wolters:j_stat_softw:15} uses a genetic algorithm
to choose a fixed-size subset under a user-supplied objective
function.  Procedures for regularized estimation of generalized linear
models with elastic-net penalties are implemented in package
\pkg{glmnet}~\citep{friedman:j_stat_softw:10}.

Here, the \pkg{lmSubsets} package~\citep{lmSubsets} for exact
variable-subset regression is presented.  It offers methods for
solving both the best-subset~\eqref{eq:best_subset} and the
all-subsets~\eqref{eq:all_subsets} selection problems.  It implements
the algorithms presented by~\citet{gatu:j_comput_graph_stat:06}
and~\citet{hofmann:comput_stat_data_an:07}.  A branch-and-bound
strategy is employed to reduce the size of the search space.  A
similar approach has been employed for exact least-trimmed-squares
regression~\citep{hofmann:j_comput_graph_stat:10}.  The package
further proposes approximation methods that compute non-exact
solutions very quickly: The exigencies toward solution quality are
relaxed by means of a tolerance parameter that steers the permitted
degree of error.  The core of the package is written in {\CXX}.  The
package is available for the {\R} environment for statistical
computing and graphics from the Comprehensive {\R} Archive Network
(CRAN) at \url{https://CRAN.R-project.org/package=lmSubsets}.

Section~\ref{sec:comput} reviews the theoretical background and the
underlying algorithms.  The package's {\R} interface is presented in
Section~\ref{sec:R}.  A usage example is given in
Section~\ref{sec:usecase}, while benchmark results are illustrated in
Section~\ref{sec:benchmarks}.

\vspace*{-0.3cm}

\section{Computational strategies}
\label{sec:comput}

The linear regression model~\eqref{eq:olm} has $2^N$ possible subset
models which can be efficiently organized in a regression tree.  A
dropping column algorithm (DCA) was devised as a straight-forward
approach to solve the all-subsets selection
problem~\eqref{eq:all_subsets}.  The DCA evaluates all possible
variable subsets by traversing a regression tree consisting of
$2^{(N-1)}$
nodes~\citep{gatu:parallel_comput:03,gatu:comput_stat_data_an:07,smith:comput_stat_data_an:89}.

Each node of the regression tree can be represented by a pair $(S,k)$,
where $S=\{s_1,\ldots,s_n\}$ corresponds to a subset of $n$ variables,
$n=0,\ldots,N$, and $k=0,\ldots,n-1$.  The subleading models are
defined as $\{s_1,\ldots,s_{k+1}\}, \ldots, \{s_1,\ldots,s_n\}$, the
RSS of which are computed for each visited node.  The root node
$(V,0)$ corresponds to the full model.  Child nodes are generated by
dropping (deleting) a single variable:
%
\begin{equation*}
  \drop(S,j)=(S\setminus\{s_j\},j-1)\text{ ,}
  \quad\text{where}\quad
  j=k+1,\ldots,n-1\text{ .}
\end{equation*}
%
Numerically, this is equivalent to downdating an orthogonal matrix
decomposition after a column has been
deleted~\citep{golub:96,kontoghiorghes:00,smith:comput_stat_data_an:89}.
Givens rotations are employed to efficiently move from one node to
another.  The DCA maintains a subset table $r$ with $N$ entries, where
entry $r_n$ contains the RSS of the current-best submodel of size
$n$~\citep{gatu:j_comput_graph_stat:06,hofmann:comput_stat_data_an:07}.
Figure~\ref{fig:dca_tree} illustrates a regression tree for $N=5$
variables.  The index $k$ is symbolized by a bullet ($\bullet$).  The
subleading models are listed in each node.

\begin{figure}[t!]
  \def\node#1#2{\TR{\psframebox[framearc=0.5]{\sffamily${\text{#1}\atop\text{#2}}$}}}
  \begin{center}
    \pstree[levelsep=8ex,treesep=2.5ex,edge=\ncline]{%
      \node{$\bullet$12345}{1, 12, 123, 1234, 12345}}{%
      \pstree{%
        \node{$\bullet$2345}{2, 23, 234, 2345}}{%
        \pstree{%
          \node{$\bullet$345}{3, 34, 345}}{%
          \pstree{%
            \node{$\bullet$45}{4, 45}}{%
            \node{$\bullet$5}{5}}
          \node{3$\bullet$5}{35}}
        \pstree{%
          \node{2$\bullet$45}{24, 245}}{%
          \node{2$\bullet$5}{25}}
        \node{23$\bullet$5}{235}}
      \pstree{%
        \node{1$\bullet$345}{13, 134, 1345}}{%
        \pstree{%
          \node{1$\bullet$45}{14, 145}}{%
          \node{1$\bullet$5}{15}}
        \node{13$\bullet$5}{135}}
      \pstree{%
        \node{12$\bullet$45}{124, 1245}}{%
        \node{12$\bullet$5}{125}}
      \node{123$\bullet$5}{1235}}
  \end{center}
  \caption{All-subsets regression tree for $N=5$ variables.  Nodes are
    shown together with their subleading models.}
  \label{fig:dca_tree}
\end{figure}

The DCA is computationally demanding, with a theoretical time
complexity of $O(2^N)$.  A branch-and-bound algorithm (BBA) has been
devised to reduce the number of generated nodes by cutting subtrees
which do not contribute to the current-best solution.  It relies on
the fundamental property that the RSS increases when variables are
deleted from a regression model, that is:
%
\begin{align*}
  S_1\subseteq S_2\implies\rss(S_1)\geq\rss(S_2)\text{ .}
\end{align*}
%
A cutting test is employed to determine which parts of the DCA tree
are redundant: A new node $\drop(S,j)$ is generated only if
$\rss(S)<r_j$ ($j=k+1,\ldots,n-1$).  The quantity $\rss(S)$ is called
the bound of the subtree rooted in $(S,k)$: No subset model extracted
from the subtree can have a smaller
RSS~\citep{gatu:j_comput_graph_stat:06}.  Note that the BBA is an
exact algorithm, i.e., it computes the optimal solution of the
all-subsets regression problem~\eqref{eq:all_subsets}.

To further reduce the computational cost, the all-subsets regression
problem can be restricted to a range of submodel
sizes~\citep{hofmann:comput_stat_data_an:07}.  In this case, the
problem~\eqref{eq:all_subsets} is reformulated as
%
\begin{equation}
  \label{eq:all_subsets:subrange}
  %
  S^*_n=\argmin_{\card{S}=n}\rss(S)
  \quad\text{for}\quad
  n=n_\text{min},\dots,n_\text{max}\text{ ,}
\end{equation}
%
where $n_\text{min}$ and $n_\text{max}$ are the subrange limits
($1\leq n_\text{min}\leq n_\text{max}\leq N$).  The search will span
only a part of the DCA regression tree.  Specifically, nodes $(S,k)$
are not computed if $\card{S}<n_\text{min}$ or $k\geq n_\text{max}$.

The size of subtrees rooted in the same level decreases exponentially
from left to right.  In order to encourage the pruning of large
subtrees by the BBA cutting test, the variables in a given node can be
ordered such that a child node will always have a larger RSS (i.e.,
bound) than its right siblings~\citep{gatu:j_comput_graph_stat:06}.
This strategy can be applied in nodes of arbitrary depth.  However,
computing the variable bounds incurs a computational overhead.  Thus,
it is not advisable to indiscriminately preorder variables.  A
parameter -- the preordering radius $p$ -- has been introduced to
control the degree of
preordering~\citep{hofmann:comput_stat_data_an:07}.  It accepts a
value between $p=0$ (no preordering) and $p=N$ (preordering in all
nodes); when $p=1$, preordering is performed in the root node only.

The computational efficiency of the BBA is improved by allowing the
algorithm to prune non-redundant branches of the regression tree.  The
approximation branch-and-bound algorithm (ABBA) relaxes the cutting
test by employing a set of tolerance parameters $\tau_n\ge 0$
($n=1,\ldots,N$), one for every submodel size.  A node $\drop(S,j)$ is
generated only if there exists at least one $i=j,\ldots,n-1$ such that
%
\begin{equation}
  \label{eq:abba}
  %
  (1+\tau_i)\cdot(\rss(S)-\rss_{\text{full}})<(r_i-\rss_{\text{full}})\text{ ,}
\end{equation}
%
where $\rss_{\text{full}}=\rss(V)$ is the RSS of the full model.  The
algorithm is non-exact if $\tau_n>0$ for any $n$, meaning that the
computed solution is not guaranteed to be optimal.  The greater the
value of $\tau_n$, the more aggressively the regression tree will be
pruned, thus decreasing the computational load.  The advantage of the
ABBA over heuristic algorithms is that the relative error of the
solution is bounded by the tolerance
parameter~\citep{gatu:j_comput_graph_stat:06,hofmann:comput_stat_data_an:07},
thus giving the user control over the tradeoff between solution
quality and speed of execution.

The DCA and its derivatives report the $N$ subset models with the
lowest RSS, one for each subset size.  The user can then analyze the
list of returned subsets to determine the \qq{best} subset, for
example by evaluating some criterion function.  This approach is
practical but not necessarily the most efficient to solve the
best-subset selection problem~\eqref{eq:best_subset}.  Let $f$ be a
criterion function such that $f(S)=F(n,\rho)$, where $n=\card{S}$ and
$\rho=\rss(S)$, satisfying the monotonicity
property~\eqref{eq:f:monotonicity}.  The $f$-BBA specializes the
standard cutting test for $f$ under the additional condition that $F$
is non-decreasing in $n$.  Specifically, a node $\drop(S,j)$ is
generated if and only if
%
\begin{equation}
  \label{eq:bba+}
  %
  F(j,\rss(S))<r_f\text{ ,}
\end{equation}
%
where $r_f$ is the single current-best solution.  This results in a
more \qq{informed} cutting test, and in a smaller number of generated
nodes.



%--------------------------------------------------------------------%
% section:  Implementation in R                                      %
%--------------------------------------------------------------------%

\section[Implementation in R]{Implementation in {\R}}
\label{sec:R}

The {\R} package \pkg{lmSubsets} provides a library of methods for
variable subset selection in linear regression.  Two {\SSS} classes
are defined, namely \class{lmSubsets} and \class{lmSelect}, that
address all-subsets~\eqref{eq:all_subsets} and
best-subset~\eqref{eq:best_subset} selection, respectively.  The
package offers {\R}'s standard formula interface: Linear models can be
specified by means of a symbolic formula, and possibly a data frame.
The model specification is resolved into a regressor matrix and a
response vector, which are forwarded to low-level functions for actual
processing, together with optional arguments which further specify the
selection problem.  A routine to extract the best submodels from an
all-subsets regression solution (i.e., to convert an \class{lmSubsets}
to an \class{lmSelect} object) is also provided.  An overview of the
package structure is given in Table~\ref{tab:structure}.

\begin{table}[b!]
  \centering
  \begin{tabular}{lll}
    \hline
    {\SSS} class           & Methods and functions    & Description                              \\
    \hline
    \class{lmSubsets}      & \fct{lmSubsets}          & All-subsets selection (generic function) \\
                           & \fct{lmSubsets.matrix}   & Matrix interface                         \\
                           & \fct{lmSubsets.default}  & Standard formula interface               \\
                           & \fct{lmSubsets\_fit}     & Low-level matrix interface               \\
    \hline
    \class{lmSelect}       & \fct{lmSelect}           & Best-subset selection (generic function) \\
                           & \fct{lmSelect.lmSubsets} & Conversion method                        \\
                           & \fct{lmSelect.matrix}    & Matrix interface                         \\
                           & \fct{lmSelect.default}   & Standard formula interface               \\
                           & \fct{lmSelect\_fit}      & Low-level matrix interface               \\
    \hline
  \end{tabular}
  \caption{Package structure.}
  \label{tab:structure}
\end{table}


%--------------------------------------------------------------------%
% section:  Specifying the problem                                   %
%--------------------------------------------------------------------%

\subsection{Specifying the selection problem}
\label{sec:specifying}

The default methods are closely modeled after {\R}'s standard \fct{lm}
function: They can be called with any entity that can be coerced to a
`\code{formula}' object~\citep{chambers:92}.  The `\code{formula}' object
declares the dependent and independent variables, which are typically
taken from a \code{data.frame} specified by the user.  For example,
the call
%
\begin{Code}
lmSubsets(mortality ~ precipitation + temperature1 + temperature7 + age +
  household + education + housing + population + noncauc + whitecollar +
  income + hydrocarbon + nox + so2 + humidity, data = AirPollution)
\end{Code}
%
specifies a response variable (\code{mortality}) and fifteen predictor
variables, all taken from the \code{AirPollution}
dataset~\citep{miller:02}.  It is common to shorten the call by
employing {\R}'s practical \qq{dot-notation}:
%
\begin{Code}
lmSubsets(mortality ~ ., data = AirPollution)
\end{Code}
%
where the dot (\code{.}) stands for \qq{all variables not mentioned in
  the left-hand side of the formula}.  By default, an intercept term
is included in the model; that is, the call in the previous example is
equivalent to
%
\begin{Code}
lmSubsets(mortality ~ . + 1, data = AirPollution)
\end{Code}
%
To discard the intercept, the call may be rewritten as follows:
%
\begin{Code}
lmSubsets(mortality ~ . - 1, data = AirPollution)
\end{Code}
%
Submodels can be rejected based on the presence or absence of certain
independent variables.  The parameter \code{include} specifies that
all submodels must contain one or several variables.  In the following
example, only submodels containing the variable \code{noncauc} are
retained:
%
\begin{Code}
lmSubsets(mortality ~ ., include = "noncauc", data = AirPollution)
\end{Code}
%
Conversely, the \code{exclude} parameter can be employed to discard a
specific set of variables, as in the following example:
%
\begin{Code}
lmSubsets(mortality ~ ., exclude = "whitecollar", data = AirPollution)
\end{Code}
%
The same effect can be achieved by rewriting the formula as follows:
%
\begin{Code}
lmSubsets(mortality ~ . - whitecollar, data = AirPollution)
\end{Code}
%
The \code{include} and \code{exclude} parameters may be used in
combination, and both may specify more than one variable
(e.g., \code{include = c("noncauc", "whitecollar")}).

The criterion used for best-subset selection is evaluated following
the expression
%
\begin{equation*}
  -2\cdot\code{logLik} + \code{penalty}\cdot\code{npar}\text{ ,}
\end{equation*}
%
where \code{penalty} is the penalty per model parameter defined
in~\eqref{eq:aic}, \code{logLik} the log-likelihood of the fitted
model, and \code{npar} the number of model parameters (including the
error variance).  The \code{penalty} value indicates how strongly
model parameters are penalized, with large values favoring
parsimonious models.  When $\code{penalty}=2$, the criterion
corresponds to Akaike's information
criterion~\citep[AIC,][]{akaike:ieee_t_automat_contr:74}; when
$\code{penalty}=\log(\code{nobs})$, to Schwarz's Bayesian information
criterion~\citep[BIC,][]{schwarz:ann_stat:78}, where \code{nobs} is
the number of observations.  For example, either one of
%
\begin{Code}
lmSelect(mortality ~ ., data = AirPollution, penalty = 2)
\end{Code}
%
and
%
\begin{Code}
lmSelect(mortality ~ ., data = AirPollution, penalty = "AIC")
\end{Code}
%
will select the best submodel according to the usual AIC; by default,
\fct{lmSelect} employs the BIC.  The user may also specify a custom
criterion function
%
\begin{Code}
lmSelect(..., penalty = function (size, rss) ...)
\end{Code}
%
where \code{size} is the number of regressors, and \code{rss} the
residual sum of squares of the corresponding submodel.  The
user-specified function must be non-decreasing in both parameters.


%--------------------------------------------------------------------%
% section:  Core functions                                           %
%--------------------------------------------------------------------%

\subsection{Core functions}
\label{sec:core}

The high-level interface methods process the model specification
before dispatching the call to one of two low-level core functions,
passing along a regressor matrix \code{x} and a response vector
\code{y}, together with problem-specific arguments.  The core
functions act as wrappers around the {\CXX} library, and are declared
as
%
\begin{Code}
lmSubsets_fit(x, y, weights = NULL, offset = NULL, include = NULL,
  exclude = NULL, nmin = NULL, nmax = NULL, tolerance = 0, nbest = 1, ...,
  pradius = NULL)
\end{Code}
%
and
%
\begin{Code}
lmSelect_fit(x, y, weights = NULL, offset = NULL, include = NULL,
  exclude = NULL, penalty = "BIC", tolerance = 0, nbest = 1, ...,
  pradius = NULL)
\end{Code}
%
The parameters are summarized in Table~\ref{tab:params}.

\begin{table}[t!]
  \centering
  \begin{tabular}{llll}
    \hline
    Parameter        & Description               & Canonical representation                         \\
    \hline
    \code{x}         & Data matrix               & \code{double[nobs, nvar]}                        \\
    \code{y}         & Response variable         & \code{double[nobs]}                              \\
    \code{weights}   & Model weights             & \code{double[nobs]}                              \\
    \code{offset}    & Model offset              & \code{double[nvar]}                              \\
    \code{include}   & Regressors to force in    & \code{logical[nvar]}                             \\
    \code{exclude}   & Regressors to force out   & \code{logical[nvar]}                             \\
    \code{nmin}      & Min.~number of regressors & \code{integer[1]}         & \fct{lmSubsets} only \\
    \code{nmax}      & Max.~number of regressors & \code{integer[1]}         & \fct{lmSubsets} only \\
    \code{penalty}   & Penalty per parameter     & \code{double[1]}          & \fct{lmSelect} only  \\
                     & or criterion function     & \code{function[1]}        &  		    \\
    \code{tolerance} & ABBA tolerance parameter  & \code{double[nvar]}       & \fct{lmSubsets}      \\
                     &                           & \code{double[1]}          & \fct{lmSelect}	    \\
    \code{nbest}     & Number of best subsets    & \code{integer[1]}                                \\
    \code{pradius}   & Preordering radius        & \code{integer[1]}                                \\
    \hline
  \end{tabular}
  \caption{Core parameters for \fct{lmSubsets} and \fct{lmSelect}.}
  \label{tab:params}
\end{table}

The \code{weights} and \code{offset} parameters correspond to the
homonymous parameters of the \fct{lm} function.  The \code{include}
and \code{exclude} parameters allow the user to specify variables that
are to be included in, or excluded from all candidate models.  They
are either logical vectors -- with each entry corresponding to one
variable -- or automatically expanded if given in the form of an
integer vector (i.e., set of variable indices) or character vector
(i.e., set of variable names).

For a large number of variables (see Section~\ref{sec:benchmarks}),
execution times may become prohibitive.  In order to speed up the
execution, either the search space can be reduced, or one may settle
for a non-exact solution.  In the first approach, the user may specify
values for the \code{nmin} and \code{nmax} parameters as defined
in~\eqref{eq:all_subsets:subrange}, in which case submodels with less
than \code{nmin} or more than \code{nmax} variables are discarded.
Well-defined regions of the regression tree can be ignored by the
selection algorithm, and the search space is thus reduced.

In the second approach, expectations with respect to the solution
quality are lowered, i.e., non-optimal solutions are tolerated.  The
numeric value -- typically between $0$ and $1$ -- passed as the
\code{tolerance} argument indicates the degree of \qq{over-pruning}
performed by the ABBA cutting test~\eqref{eq:abba}.  The solution
produced by the ABBA satisfies the following relationship:
%
\begin{equation*}
  f(S)-f(V)\leq(1+\code{tolerance})\cdot (f(S^*)-f(V))\text{ ,}
\end{equation*}
%
where $S$ is the returned solution, $V$ the full model, $S^*$ the
optimal (theoretical) solution, and $f$ the cost of a submodel (e.g.,
deviance, AIC).  The \fct{lmSubsets\_fit} function accepts a vector of
tolerances, with one entry for each subset size.

The \code{nbest} parameter controls how many submodels (per subset size) are retained.
In the case of \fct{lmSubsets\_fit}, a two-dimensional result set is
constructed with \code{nbest} submodels for each subset size, while in
the case of \fct{lmSelect\_fit}, a one-dimensional sequence of
\code{nbest} submodels is handed back to the user.

The \code{pradius} parameter serves to specify the desired preordering
radius.  The algorithm employs a default value of
$\lfloor\code{nvar}/3\rfloor$.  The need to set this parameter
directly should rarely arise; please refer to Section~\ref{sec:comput}
for further information.

%--------------------------------------------------------------------%
% section:  Extracting submodels                                     %
%--------------------------------------------------------------------%

\subsection{Extracting submodels}
\label{sec:extracting}

The user is handed back a result object that encapsulates the solution
to an all-subsets (class \class{lmSubsets}) or best-subset (class
\class{lmSelect}) selection problem.  An object of class
\class{lmSubsets} represents a two-dimensional
$\code{nvar}\times\code{nbest}$ set of submodels; an object of class
\class{lmSelect}, a linear sequence of \code{nbest} submodels.
Problem-specific information is stored alongside the selected
submodels.  Table~\ref{tab:components} summarizes the components of
the result objects.

\begin{table}[t!]
  \centering
  \begin{tabular}{lll}
    \hline
    Component        & Description            & Canonical representation \\
    \hline        
    \code{nobs}      & Number of observations & \code{integer[1]}        \\
    \code{nvar}      & Number of regressors   & \code{integer[1]}        \\
    \code{intercept} & Intercept flag         & \code{logical[1]}        \\
    \code{include}   & Regressors forced in   & \code{logical[nvar]}     \\
    \code{exclude}   & Regressors forced out  & \code{logical[nvar]}     \\
    \code{size}      & Covered subset sizes   & \code{integer[]}          \\
    \code{tolerance} & Tolerances used        & \code{double[nvar]}      \\
    \code{nbest}     & Number of best subsets & \code{integer[1]}        \\
    \code{submodel}  & Submodel information   & \code{data.frame}        \\
    \code{subset}    & Selected variables     & \code{data.frame}        \\
    \hline
  \end{tabular}
  \caption{Components of \class{lmSubsets} and \class{lmSelect} objects.}
  \label{tab:components}
\end{table}

A wide range of standard methods to visualize, summarize, and extract
information are provided (see Table~\ref{tab:methods}).  The
\fct{print}, \fct{plot}, and \fct{summary} methods give the user a
compact overview -- either textual or graphical -- of the information
gathered on the selected submodels in order to help identify \qq{good}
candidates.  The remaining extractor functions can be used to extract
variable names, coefficients, covariance matrices, fitted values, etc.

In order to designate a submodel, \class{lmSubsets} methods provide
two parameters to specify the number of regressors and the ranking of
the desired submodel, namely \code{size} and \code{best},
respectively.  For \class{lmSelect} methods, the \code{size} parameter
has no meaning and is not defined.  Some methods -- i.e.,
\fct{variable.names}, \fct{deviance}, \fct{sigma}, \fct{logLik},
\fct{AIC}, \fct{BIC}, and \fct{coef} -- can extract more than one
submodel at a time if passed a numeric vector as an argument to either
\code{size} (e.g., \code{size = 5:10}) or \code{best} (e.g.,
\code{best = 1:3}).  The shape of the return value can be controlled
with the \code{drop} parameter: a \code{numeric} or \code{character}
vector (in some cases, a \code{logical} or \code{numeric} matrix) is
returned if \code{drop} is \code{TRUE}; otherwise, a \code{data.frame}
object is handed back.

\begin{table}[t!]
  \centering
  \begin{tabular}{ll}
    \hline
    Method                & Description                         \\
    \hline
    \fct{print}           & Print object                        \\
    \fct{plot}            & Plot RSS or penalty                 \\
    \fct{image}           & Heatmap of selected regressors      \\
    \fct{summary}         & Summary statistics                  \\
    \hline
    \fct{variable.names}  & Extract variables names             \\
    \fct{formula}         & Extract formula object              \\
    \fct{model.frame}     & Extract (full) model frame          \\
    \fct{model.matrix}    & Extract model matrix                \\
    \fct{model\_response} & Extract model response              \\
    \fct{refit}           & Fit sub-\class{lm}                  \\
    \fct{deviance}        & Extract deviance (RSS)              \\
    \fct{sigma}           & Extract residual standard deviation \\
    \fct{logLik}          & Extract log-likelihood              \\
    \fct{AIC}             & Extract AIC values                  \\
    \fct{BIC}             & Extract BIC values                  \\
    \fct{coef}            & Extract regression coefficients     \\
    \fct{vcov}            & Extract covariance matrix           \\
    \fct{fitted}          & Extract fitted values               \\
    \fct{residuals}       & Extract residual values             \\
    \hline
  \end{tabular}
  \caption{{\SSS} methods for \class{lmSubsets} and \class{lmSelect}
    objects.}
  \label{tab:methods}
\end{table}


%--------------------------------------------------------------------%
% section:  Case study                                               %
%--------------------------------------------------------------------%

\section{Case study: Variable selection in weather forecasting}
\label{sec:usecase}

Advances in numerical weather prediction (NWP) have played an
important role in the increase of weather forecast skill over the past
decades~\citep{bauer:nature:15}.  Numerical models simulate physical
systems that operate at a large, typically global, scale.  The
horizontal (spatial) resolution is limited by the computational power
available today. Starting from \cite{glahn:j_appl_meteorol:72}
the NWP outputs are post-processed to correct for local and unresolved
effects in order to obtain forecasts for specific locations
\citep[see][Chapter~7, for an overview]{wilks:11}. So-called model
output statistics (MOS) develops a regression relationship based
on past meteorological observations of the variable to be predicted
and forecasted NWP quantities at a certain lead time. Variable-subset
selection is often employed to determine which NWP outputs should be
included in the regression model for a specific location.

In the following, package \pkg{lmSubsets} is used to build a MOS
regression model predicting temperature at Innsbruck Airport, Austria,
based on data from the Global Ensemble Forecast
System~\citep{hamill:b_am_meteorol_soc:13}.  The data frame
\code{IbkTemperature} contains 1824 daily cases for 42 variables: the
temperature at Innsbruck Airport (observed), 36 NWP outputs
(forecasted), and 5 deterministic time trend/season patterns.  The NWP
variables include quantities pertaining to temperature (e.g., 2-meter
above ground, minimum, maximum, soil), precipitation, wind, and
fluxes, among others.  See \code{?IbkTemperature} for more details.

First, the dataset is loaded and the few missing values are omitted for
simplicity.
%
\begin{Schunk}
\begin{Sinput}
R> data("IbkTemperature", package = "lmSubsets")
R> IbkTemperature <- na.omit(IbkTemperature)
\end{Sinput}
\end{Schunk}
%
A simple output model for the observed temperature (\code{temp}) is
constructed, which will serve as the reference model.  It consists of
the 2-meter temperature NWP forecast (\code{t2m}), a linear trend
component (\code{time}), as well as seasonal components with annual
(\code{sin}, \code{cos}) and bi-annual (\code{sin2}, \code{cos2})
harmonic patterns.
%
\begin{Schunk}
\begin{Sinput}
R> MOS0 <- lm(temp ~ t2m + time + sin + cos + sin2 + cos2,
+    data = IbkTemperature)
\end{Sinput}
\end{Schunk}
%
The estimated coefficients (and standard errors) are shown in
Table~\ref{tab:summary}.  It can be observed that despite the
inclusion of the NWP variable \code{t2m}, the coefficients for the
deterministic components remain significant, which indicates that the
seasonal temperature fluctuations are not fully resolved by the
numerical model.

\begin{table}[t!]
\centering
\small
\begin{tabular}{>{\ttfamily}l*{3}{%
      >{\ttfamily}r%
      @{}>{\ttfamily}l%
      @{ }>{\ttfamily}r}}
\hline
&
\multicolumn{3}{@{}c}{\ttfamily MOS0} &
\multicolumn{3}{@{}c}{\ttfamily MOS1} &
\multicolumn{3}{@{}c}{\ttfamily MOS2} \\
\hline
(Intercept) & -345.252 & ** & (109.212) & -666.584 & *** & (95.349) & -661.700 & *** & (95.225)\\
t2m & 0.318 & *** & (~~0.016) & 0.055 & . & (~0.029) &  &  & \\
time & 0.132 & * & (~~0.054) & 0.149 & ** & (~0.047) & 0.147 & ** & (~0.047)\\
sin & -1.234 & *** & (~~0.126) & 0.522 & *** & (~0.147) & 0.811 & *** & (~0.120)\\
cos & -6.329 & *** & (~~0.164) & -0.812 & ** & (~0.273) &  &  & \\
sin2 & 0.240 & * & (~~0.110) & -0.794 & *** & (~0.119) & -0.870 & *** & (~0.118)\\
cos2 & -0.332 & ** & (~~0.109) & -1.067 & *** & (~0.101) & -1.128 & *** & (~0.097)\\
sshnf &  &  &  & 0.016 & *** & (~0.004) & 0.018 & *** & (~0.004)\\
vsmc &  &  &  & 20.200 & *** & (~3.115) & 20.181 & *** & (~3.106)\\
tmax2m &  &  &  & 0.145 & *** & (~0.037) & 0.181 & *** & (~0.023)\\
st &  &  &  & 1.077 & *** & (~0.051) & 1.142 & *** & (~0.043)\\
wr &  &  &  & 0.450 & *** & (~0.109) & 0.505 & *** & (~0.103)\\
t2pvu &  &  &  & 0.064 & *** & (~0.011) & 0.149 & *** & (~0.028)\\
mslp &  &  &  &  &  &  & -0.000 & *** & (~0.000)\\
p2pvu &  &  &  &  &  &  & -0.000 & ** & (~0.000)\\
\hline
AIC  &  9493.602  &   &   &  8954.907  &   &   &  8948.182  &   &  \\ 
BIC  &  9537.650  &   &   &  9031.992  &   &   &  9025.267  &   &  \\ 
RSS  &  19506.469  &   &   &  14411.122  &   &   &  14357.943  &   &  \\ 
Sigma  &  3.281  &   &   &  2.825  &   &   &  2.820  &   &  \\ 
R-sq.  &  0.803  &   &   &  0.854  &   &   &  0.855  &   &  \\ 
\hline
\end{tabular}
\caption{Estimated regression coefficients (along with standard
  errors) and summary statistics for models \code{MOS0}, \code{MOS1},
  and \code{MOS2}.}
\label{tab:summary}%
\end{table}

Next, the reference model is extended with selected regressors taken
from the remaining 35~NWP variables.
%
\begin{Schunk}
\begin{Sinput}
R> MOS1_best <- lmSelect(temp ~ ., data = IbkTemperature,
+    include = c("t2m", "time", "sin", "cos", "sin2", "cos2"),
+    penalty = "BIC", nbest = 20)
R> MOS1 <- refit(MOS1_best)
\end{Sinput}
\end{Schunk}
%
Best-subset regression is employed to determine pertinent variables in
addition to the regressors already found in \code{MOS0}.  The 20 best
submodels with respect to the BIC are computed.  The selected subsets
and the corresponding BIC values are illustrated in
Figures~\ref{fig:image:mos1} and~\ref{fig:plot:mos1}, respectively.
The \class{lm} object for the best submodel is extracted
(\code{MOS1}).  Selected coefficients and summary statistics for
\code{MOS1} are listed in Table~\ref{tab:summary}.

\begin{figure}[t!]
\centering
\begin{subfigure}[b]{\textwidth}
\setkeys{Gin}{width=\textwidth}
\includegraphics[trim=0 10 0 20, clip]{jss3444-mos1-best-image}
\caption{\code{MOS1\_best}}
\label{fig:image:mos1}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\setkeys{Gin}{width=\textwidth}
\includegraphics[trim=0 10 0 20, clip]{jss3444-mos2-all-image}
\caption{\code{MOS2\_all}}
\label{fig:image:mos2}
\end{subfigure}
\caption{Variables selected in \code{MOS1\_best} and \code{MOS2\_all}.
  Submodels \code{MOS1} and \code{MOS2} are highlighted in red.}
\end{figure}

\begin{figure}[t!]
\centering
\begin{subfigure}[b]{0.5\textwidth}
\setkeys{Gin}{width=\textwidth}
\includegraphics[trim = 0 10 0 22, clip]{jss3444-mos1-best-plot}
\caption{\code{MOS1\_best}}
\label{fig:plot:mos1}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\setkeys{Gin}{width=\textwidth}
\includegraphics[trim = 0 10 0 22, clip]{jss3444-mos2-all-plot}
\caption{\code{MOS2\_all}}
\label{fig:plot:mos2}
\end{subfigure}
\caption{BIC (and RSS) for submodels in \code{MOS1\_best} and
  \code{MOS2\_all}.}
\end{figure}

Finally, an all-subsets regression is conducted on all 41 variables
without any restrictions.
%
\begin{Schunk}
\begin{Sinput}
R> MOS2_all <- lmSubsets(temp ~ ., data = IbkTemperature)
R> MOS2 <- refit(lmSelect(MOS2_all, penalty = "BIC"))
\end{Sinput}
\end{Schunk}
%
The results are illustrated in Figures~\ref{fig:image:mos2}
and~\ref{fig:plot:mos2}.  Here, all-subsets regression is employed --
instead of the cheaper best-subsets regression -- in order to give
insights into possible variable selection patterns over a range of
submodel sizes.  The \class{lm} object for the submodel with the
lowest BIC is extracted (\code{MOS2}).  See Table~\ref{tab:summary}
for \code{MOS2} summary statistics.

The best-BIC models \code{MOS1} and \code{MOS2} both have 13
regressors.  The deterministic trend and all but one of the harmonic
seasonal components are retained in \code{MOS2}.  In addition,
\code{MOS1} and \code{MOS2} share six NWP outputs relating to
temperature (\code{tmax2m}, \code{st}, \code{t2pvu}), pressure
(\code{mslp}, \code{p2pvu}), hydrology (\code{vsmc}, \code{wr}), and
heat flux (\code{sshnf}).  However, and most remarkably, \code{MOS1}
does not include the direct 2-meter temperature output from the NWP
model (\code{t2m}).  In fact, \code{t2m} is not included by any of the
20 submodels (sizes 8 to 27) shown in Figure~\ref{fig:image:mos2},
whereas the temperature quantities \code{tmax2m}, \code{st},
\code{t2pvu} are included by all.  The summary statistics reveal that
both \code{MOS1} and \code{MOS2} significantly improve over the simple
reference model \code{MOS0}, with \code{MOS2} being slightly better
than \code{MOS1}.

In summary, this case study illustrates how \pkg{lmSubsets} can be
used to easily identify relevant variables beyond the direct model
output for MOS regressions, yielding substantial improvements in
forecasting skill.  A full meteorological application would require
further testing using cross-validation or other out-of-sample
assessments.  Recently, there has been increasing interest in MOS
models beyond least-squares linear regression, e.g., to take into
account the effects of heteroscedasticity, censoring, and truncation.
In this context, other selection techniques -- such as
boosting~\citep{messner:r_j:16,messner:mon_weather_rev:17} -- can be
considered.


\vspace*{-0.25cm}

\section{Benchmark tests}
\label{sec:benchmarks}

\begin{table}[t!]
  \centering
  \begin{threeparttable}
  \begin{tabular}{*{7}{>{\ttfamily}r}}
    \hline
    sigma & nvar &
    \multicolumn{2}{c}{\pkg{leaps}} &
    \multicolumn{3}{c}{\pkg{lmSubsets}} \\ \cline{3-4}\cline{5-7}
    & &
    \fct{regsubsets}\tnote{1} & \fct{regsubsets}\tnote{2} &
    \fct{lmSubsets} & \rmfamily{speedup}\tnote{1} & \rmfamily{speedup}\tnote{2} \\
\hline
0.05 & 20 & 0.009\,s & 0.004\,s & 0.021\,s & 0.4 & 0.2\\
 & 25 & 0.072\,s & 0.033\,s & 0.011\,s & 6.4 & 2.9\\
 & 30 & 0.829\,s & 0.474\,s & 0.027\,s & 31.2 & 17.8\\
 & 35 & 12.309\,s & 5.084\,s & 0.067\,s & 182.6 & 75.4\\
 & 40 & 172.613\,s & 82.566\,s & 0.313\,s & 550.8 & 263.5\\
\hline
0.10 & 20 & 0.008\,s & 0.004\,s & 0.007\,s & 1.1 & 0.6\\
 & 25 & 0.064\,s & 0.031\,s & 0.010\,s & 6.2 & 3.0\\
 & 30 & 0.970\,s & 0.457\,s & 0.027\,s & 36.5 & 17.2\\
 & 35 & 9.912\,s & 4.792\,s & 0.068\,s & 146.2 & 70.7\\
 & 40 & 208.998\,s & 93.101\,s & 0.334\,s & 626.5 & 279.1\\
\hline
0.50 & 20 & 0.009\,s & 0.004\,s & 0.007\,s & 1.2 & 0.6\\
 & 25 & 0.081\,s & 0.031\,s & 0.011\,s & 7.5 & 2.9\\
 & 30 & 0.995\,s & 0.462\,s & 0.026\,s & 38.0 & 17.6\\
 & 35 & 12.751\,s & 4.995\,s & 0.068\,s & 187.5 & 73.4\\
 & 40 & 204.834\,s & 82.710\,s & 0.312\,s & 656.9 & 265.3\\
\hline
1.00 & 20 & 0.008\,s & 0.004\,s & 0.007\,s & 1.2 & 0.6\\
 & 25 & 0.070\,s & 0.033\,s & 0.011\,s & 6.6 & 3.1\\
 & 30 & 0.971\,s & 0.461\,s & 0.026\,s & 37.6 & 17.9\\
 & 35 & 13.066\,s & 4.560\,s & 0.066\,s & 198.6 & 69.3\\
 & 40 & 171.499\,s & 62.978\,s & 0.277\,s & 620.0 & 227.7\\
\hline
5.00 & 20 & 0.008\,s & 0.004\,s & 0.007\,s & 1.1 & 0.5\\
 & 25 & 0.058\,s & 0.019\,s & 0.010\,s & 5.7 & 1.9\\
 & 30 & 0.588\,s & 0.198\,s & 0.021\,s & 28.5 & 9.6\\
 & 35 & 6.951\,s & 2.455\,s & 0.053\,s & 131.7 & 46.5\\
 & 40 & 117.859\,s & 30.252\,s & 0.193\,s & 609.4 & 156.4\\
    \hline
  \end{tabular}
  \begin{tablenotes}
  \item[1] \fct{regsubsets} is executed w/out preliminary preordering of the variables.
  \item[2] \fct{regsubsets} is executed with preliminary preordering of the variables.
  \end{tablenotes}
  \caption{Speedup of \fct{lmSubsets} relative to \fct{regsubsets};
    average execution times in seconds.}
  \label{tab:bm1}
  \end{threeparttable}
\end{table}

Comparative tests are conducted to evaluate the computational
efficiency of the proposed methods for exact all-subsets and exact
best-subset regression.  The \fct{regsubsets} method from package
\pkg{leaps}, and the \fct{bestglm} method from package \pkg{bestglm}
serve as benchmarks, respectively.

Datasets which contain a \qq{true} model are simulated, with
\code{nobs} observations and \code{nvar} independent variables.  The
dependent variable \code{y} is constructed from a linear combination
of \code{ntrue} randomly selected independent variables, a noise
vector \code{e}, and the intercept:
%
\begin{equation*}
  \code{y}=\code{X}\cdot\mathbbm{1}_\text{true}+\code{e}+1\text{ ,}
  \quad\text{where}\quad
  \code{e}\sim(0,\code{sigma}^2)\text{ ,}
\end{equation*}
%
where \code{X} is a $\code{nobs}\times\code{nvar}$ matrix of random
data, and $\mathbbm{1}_\text{true}$ a (random) indicator function
evaluating to 1 if the corresponding column of \code{X} belongs to the
\qq{true} model.  All tests were conducted on a Dell XPS15 laptop with
8GB (7.4 GiB) of memory and an Intel Core i7-6700HQ
CPU@2.60GHz$\times$8 processor, running a Ubuntu 64bit operating
system.

Benchmark~1 concerns itself with all-subsets selection.  The
\fct{lmSubsets} method is compared to \fct{regsubsets}.  The
complexity mainly depends on the number of variables (\code{nvar}):
The algorithms employ the QR decomposition to compress the data into a
square $\code{nvar}\times\code{nvar}$ matrix; the initial cost of
constructing the QR decomposition is negligible.  Data configurations
with varying sizes ($\code{nvar}=20,25,30,35,40$) and degrees of noise
($\code{sigma}=0.05,0.10\text{, }0.50,1.00,5.00$) are considered; in
all cases, \code{nobs} = 1000 and $\code{ntrue} =
\lfloor\code{nvar}/2\rfloor$.  For each configuration, five random
datasets are generated, giving rise to five runs per method over which
average execution times are determined.  The performance of
\fct{regsubsets} can be improved by \qq{manually} preordering the
dataset in advance~\citep{hofmann:comput_stat_data_an:07}.  The
average running times are summarized in Table~\ref{tab:bm1}, along
with the relative performance (speedup) of \fct{lmSubsets}.  The same
setup is used in Benchmark~2, where methods for best-subset selection
are compared, namely \fct{bestglm} and \fct{lmSelect}.  As in the
previous benchmark, average execution times are determined for
\fct{bestglm} with and without preordering.  The results are
illustrated in Table~\ref{tab:bm2}.

\begin{table}[t!]
  \centering
  \begin{threeparttable}
  \begin{tabular}{*{7}{>{\ttfamily}r}}
    \hline
    sigma & nvar &
    \multicolumn{2}{c}{\pkg{bestglm}} &
    \multicolumn{3}{c}{\pkg{lmSubsets}} \\ \cline{3-4}\cline{5-7}
    & &
    \fct{bestglm}\tnote{1} & \fct{bestglm}\tnote{2} &
    \fct{lmSelect} & \rmfamily{speedup}\tnote{1} & \rmfamily{speedup}\tnote{2} \\
\hline
0.05 & 20 & 0.021\,s & 0.017\,s & 0.006\,s & 3.4 & 2.7\\
 & 25 & 0.083\,s & 0.046\,s & 0.008\,s & 10.4 & 5.7\\
 & 30 & 0.835\,s & 0.489\,s & 0.008\,s & 99.4 & 58.2\\
 & 35 & 12.270\,s & 5.110\,s & 0.010\,s & 1202.9 & 501.0\\
 & 40 & 174.041\,s & 83.399\,s & 0.012\,s & 14503.4 & 6949.9\\
\hline
0.10 & 20 & 0.020\,s & 0.016\,s & 0.007\,s & 3.0 & 2.5\\
 & 25 & 0.074\,s & 0.045\,s & 0.007\,s & 10.1 & 6.1\\
 & 30 & 0.974\,s & 0.471\,s & 0.009\,s & 110.7 & 53.5\\
 & 35 & 9.875\,s & 4.777\,s & 0.010\,s & 949.5 & 459.3\\
 & 40 & 210.076\,s & 93.968\,s & 0.012\,s & 17219.3 & 7702.3\\
\hline
0.50 & 20 & 0.020\,s & 0.017\,s & 0.006\,s & 3.3 & 2.7\\
 & 25 & 0.093\,s & 0.044\,s & 0.008\,s & 12.2 & 5.8\\
 & 30 & 1.004\,s & 0.474\,s & 0.009\,s & 114.1 & 53.8\\
 & 35 & 12.744\,s & 5.067\,s & 0.011\,s & 1158.5 & 460.7\\
 & 40 & 205.744\,s & 83.268\,s & 0.012\,s & 17145.4 & 6939.0\\
\hline
1.00 & 20 & 0.021\,s & 0.017\,s & 0.006\,s & 3.2 & 2.7\\
 & 25 & 0.082\,s & 0.046\,s & 0.007\,s & 11.0 & 6.3\\
 & 30 & 0.979\,s & 0.474\,s & 0.008\,s & 119.3 & 57.8\\
 & 35 & 13.002\,s & 4.568\,s & 0.011\,s & 1182.0 & 415.2\\
 & 40 & 172.923\,s & 63.283\,s & 0.012\,s & 14907.2 & 5455.4\\
\hline
5.00 & 20 & 0.020\,s & 0.016\,s & 0.006\,s & 3.2 & 2.6\\
 & 25 & 0.070\,s & 0.032\,s & 0.008\,s & 9.3 & 4.3\\
 & 30 & 0.598\,s & 0.212\,s & 0.009\,s & 63.6 & 22.6\\
 & 35 & 6.942\,s & 2.467\,s & 0.012\,s & 588.3 & 209.1\\
 & 40 & 118.004\,s & 30.404\,s & 0.018\,s & 6555.8 & 1689.1\\
    \hline
  \end{tabular}
  \begin{tablenotes}
  \item[1] \fct{bestglm} is executed w/out preliminary preordering of the variables.
  \item[2] \fct{bestglm} is executed with preliminary preordering of the variables.
  \end{tablenotes}
  \caption{Speedup of \fct{lmSelect} relative to \fct{bestglm};
    average execution times in seconds.}
  \label{tab:bm2}
  \end{threeparttable}
\end{table}

It is not surprising that \fct{bestglm} is very close to
\fct{regsubsets} in terms of execution time, as the former
post-processes the results returned by the latter; in fact,
\fct{bestglm} implements the two-stage approach to solving the
best-subset selection problem, where Stage~1 is tackled by
\fct{regsubsets} (see Section~\ref{sec:intro} for further details).
Manually preordering the variables improves the performance of
\fct{regsubsets} (and hence, of \fct{bestglm}) by a factor of
approximately 2; for $\code{nvar}=40$ and a high level of noise
($\code{sigma}=5.00$), by a factor of almost 4.  In the tests
conducted here, \fct{lmSubsets} is two orders of magnitude faster than
\fct{regsubsets}, even with preordering; \fct{lmSelect} is three
orders of magnitude faster than \fct{bestglm}.

Benchmark~3 pits all-subsets and best-subset selection, exact and
approximation algorithms against one another.  The average execution
times of \fct{lmSubsets} and \fct{lmSelect}, for
$\code{tolerance}=0.0\text{ and }0.1$, are illustrated in
Table~\ref{tab:bm3}.  Note that for large datasets ($\code{nvar}=80$),
subsets computed by \fct{lmSubsets} are restricted to sizes between
$\code{nmin}=30$ and $\code{nmax}=50$ variables; the restriction does
not apply to \fct{lmSelect}.

\begin{table}[t!]
  \centering\scriptsize
  \begin{tabular}{*{10}{>{\ttfamily}r}}
    \hline
    sigma & nvar & nmin & nmax &
    \multicolumn{3}{c}{$\code{tolerance}=0.0$} &
    \multicolumn{3}{c}{$\code{tolerance}=0.1$} \\ \cline{5-7}\cline{8-10}
    & & & &
    \fct{lmSubsets} & \fct{lmSelect} & \rmfamily{speedup} &
    \fct{lmSubsets} & \fct{lmSelect} & \rmfamily{speedup} \\
\hline
0.05 & 20 & - & - & 0.021\,s & 0.022\,s & 1.0 & 0.006\,s & 0.007\,s & 1.0\\
 & 40 & - & - & 0.327\,s & 0.012\,s & 26.8 & 0.227\,s & 0.012\,s & 18.6\\
 & 60 & - & - & 217.028\,s & 0.020\,s & 10961.0 & 101.557\,s & 0.020\,s & 5181.5\\
 & 80 & 30 & 50 & 413.982\,s & 0.032\,s & 12936.9 & 163.629\,s & 0.031\,s & 5312.6\\
\hline
0.10 & 20 & - & - & 0.007\,s & 0.006\,s & 1.1 & 0.007\,s & 0.007\,s & 1.0\\
 & 40 & - & - & 0.315\,s & 0.012\,s & 26.2 & 0.220\,s & 0.012\,s & 18.4\\
 & 60 & - & - & 208.721\,s & 0.020\,s & 10332.7 & 97.096\,s & 0.020\,s & 4953.9\\
 & 80 & 30 & 50 & 465.824\,s & 0.031\,s & 14835.2 & 185.665\,s & 0.031\,s & 5989.2\\
\hline
0.50 & 20 & - & - & 0.007\,s & 0.007\,s & 1.0 & 0.007\,s & 0.007\,s & 1.0\\
 & 40 & - & - & 0.336\,s & 0.012\,s & 28.0 & 0.231\,s & 0.012\,s & 19.2\\
 & 60 & - & - & 197.147\,s & 0.020\,s & 9956.9 & 93.936\,s & 0.020\,s & 4744.2\\
 & 80 & 30 & 50 & 486.858\,s & 0.032\,s & 15406.9 & 195.240\,s & 0.031\,s & 6217.8\\
\hline
1.00 & 20 & - & - & 0.007\,s & 0.006\,s & 1.1 & 0.007\,s & 0.007\,s & 1.0\\
 & 40 & - & - & 0.290\,s & 0.012\,s & 24.2 & 0.205\,s & 0.012\,s & 17.1\\
 & 60 & - & - & 228.710\,s & 0.020\,s & 11668.9 & 106.009\,s & 0.019\,s & 5464.4\\
 & 80 & 30 & 50 & 374.452\,s & 0.032\,s & 11701.6 & 148.421\,s & 0.043\,s & 3467.8\\
\hline
5.00 & 20 & - & - & 0.007\,s & 0.007\,s & 1.1 & 0.007\,s & 0.007\,s & 1.0\\
 & 40 & - & - & 0.196\,s & 0.017\,s & 11.4 & 0.146\,s & 0.015\,s & 9.5\\
 & 60 & - & - & 89.244\,s & 0.195\,s & 458.1 & 47.399\,s & 0.114\,s & 416.5\\
 & 80 & 30 & 50 & 154.836\,s & 8.056\,s & 19.2 & 58.246\,s & 3.012\,s & 19.3\\
    \hline
  \end{tabular}
  \caption{Speedup of \fct{lmSelect} relative to \fct{lmSubsets}, with
    and without tolerance; average execution times in seconds.}
  \label{tab:bm3}
\end{table}

In the case of \fct{lmSubsets}, the approximation algorithm
($\code{tolerance}=0.1$) is 2--3~times faster than the exact
algorithm.  The speedup of \fct{lmSelect} with respect to
\fct{lmSubsets} is four orders of magnitude for the exact, three
orders of magnitude for the approximation algorithm.  It is
interesting to note, that the computational performance of
\fct{lmSubsets} increases for high levels of noise
($\code{sigma}=5.00$), contrary to \fct{lmSelect}.  Under these
conditions, the relative speedup of \fct{lmSelect} is significantly
lower.  As the noise increases, the information in the data is
\qq{blurred}, rendering the cutting test~\eqref{eq:bba+} -- which
depends on the information criterion -- less effective; in this
respect, \fct{lmSubsets} is more robust, as it only depends on the
RSS.

In Benchmark~4, the effects of the \code{nbest} parameter (number of
computed best submodels) on the execution times of \fct{lmSelect} are
investigated.  Two information criteria are considered
($\code{ic}$ is $\code{"AIC"}\text{ and }\code{"BIC"}$).  The noise level used
in the benchmark is $\code{sigma}=1.0$.  Average execution times are
reported in Table~\ref{tab:bm4} for $\code{nbest}=1,5,10,15,20$.
Finally, Benchmark~5 investigates how the AIC penalty per parameter
(\code{penalty}) affects the performance of \fct{lmSelect}.
Table~\ref{tab:bm5} summarizes the results for
$\code{penalty}=1.0,2.0,4.0,8.0,16.0,32.0$.  Note that
$\code{penalty}=2.0$ and $\code{penalty}=\code{log}(1000)\approx 6.9$
correspond to the usual AIC and BIC, respectively (here,
$\code{nobs}=1000$).  The results reveal that the execution time of
\fct{lmSelect} increases linearly with \code{nbest}, and -- from the
values considered here -- is minimal for $\code{penalty}=8.0$, which is
close to the BIC.

\begin{table}[t!]
  \centering
  \begin{tabular}{*{7}{>{\ttfamily}r}}
    \hline
    nvar & ic & \multicolumn{5}{c}{\ttfamily nbest} \\ \cline{3-7}
    & & 1 & 5 & 10 & 15 & 20 \\
\hline
100 & AIC & 2.159\,s & 2.334\,s & 2.457\,s & 2.557\,s & 2.639\,s\\
 & BIC & 0.051\,s & 0.058\,s & 0.068\,s & 0.074\,s & 0.079\,s\\
\hline
200 & BIC & 23.987\,s & 49.622\,s & 82.175\,s & 104.860\,s & 119.064\,s\\
    \hline
  \end{tabular}
  \caption{Average execution times (in seconds) of \fct{lmSelect}, by
    number of computed subset models (\code{nbest}).}
  \label{tab:bm4}
\end{table}

\begin{table}[t!]
  \centering
  \begin{tabular}{*{7}{>{\ttfamily}r}}
    \hline
    nvar & \multicolumn{6}{c}{\ttfamily penalty} \\ \cline{2-7}
    & 1.0 & 2.0 & 4.0 & 8.0 & 16.0 & 32.0 \\
\hline
80 & 0.293\,s & 0.300\,s & 0.050\,s & 0.033\,s & 0.032\,s & 0.068\,s\\
100 & 4.223\,s & 0.894\,s & 0.084\,s & 0.048\,s & 0.054\,s & 0.341\,s\\
120 & 14.178\,s & 6.420\,s & 0.531\,s & 0.085\,s & 0.168\,s & 3.925\,s\\
    \hline
  \end{tabular}
  \caption{Average execution times (in seconds) of \fct{lmSelect}, by
    AIC penalty per parameter (\code{penalty}).}
  \label{tab:bm5}
\end{table}


\subsection{Shrinkage methods}

Genetic algorithms for model selection have been considered for
comparative study.  However, pertinent {\R} packages have been found
to impose restrictions on the class of problems that can be
addressed -- limited problem size (\pkg{glmulti}), fixed submodel size
(\pkg{kofnGA}), or no immediate support for IC-based search
(\pkg{subselect}) --, hampering efforts to conduct a meaningful
comparison.

LASSO~\citep{tibshirani:j_roy_stat_soc_b_met:96} can be seen as an
alternative to exact variable selection methods, of which package
\pkg{glmnet} brings an efficient implementation to {\R}.  The function
\fct{glmnet} computes an entire regularization path and returns a
sequence of sparse estimators.  The method is not IC-based; rather, it
employs a modified objective function that induces sparsity by
penalizing the regression coefficients.

The return value of \fct{glmnet} can be post-processed for comparison
with \fct{lmSelect}.  For each (sparse) estimator contained in the
sequence returned by \fct{glmnet}, the subset model corresponding to
the variables with non-zero coefficients is identified; the submodel
is fitted (in the OLS sense), and the BIC extracted.  The list of
submodels thus obtained is sorted in order of increasing BIC, after
removal of duplicates.

Comparative results are illustrated in Table~\ref{tab:bmlasso}.  For
each data configuration, five datasets are simulated.  The ten best
submodels are computed by \fct{lmSelect} (i.e., $\code{nbest}=10$).
Average execution times of \fct{lmSelect} and \fct{glmnet} are
reported, as well as the average number of matches -- i.e., the number
of best subsets correctly identified -- and the speedup of the LASSO.
Each function returns an ordered sequence of submodels; the number of
matches is $k$ if and only if the two sequences are identical in the
first $k$ entries and differ in the $(k+1)$th.

The takeaway it that the LASSO is computationally very efficient; it
is much less affected by the dimension of the problem than
\fct{lmSelect}.  On the other hand, while \fct{lmSelect} always finds
the global optimum -- or a solution with provable error bounds when a
tolerance is employed --, \fct{glmnet} does not provide any guarantees
on the distance of the result from the optimal solution (in the OLS
sense).


\begin{table}[t!]
  \centering
  \begin{tabular}{*{6}{>{\ttfamily}r}}
    \hline
    sigma & nvar &
    \multicolumn{1}{c}{\pkg{lmSubsets}} &
    \multicolumn{3}{c}{\pkg{glmnet}} \\ \cline{3-3}\cline{4-6}
    & & \fct{lmSelect} &
    \fct{glmnet} & \rmfamily{speedup} & \rmfamily{matches} \\
\hline
0.05 & 60 & 0.020\,s & 0.006\,s & 3.6 & 0.8\\
 & 100 & 0.075\,s & 0.009\,s & 8.0 & 0.6\\
 & 140 & 1.012\,s & 0.016\,s & 63.2 & 0.6\\
\hline
0.10 & 60 & 0.019\,s & 0.005\,s & 4.0 & 0.8\\
 & 100 & 0.070\,s & 0.009\,s & 7.4 & 0.6\\
 & 140 & 1.148\,s & 0.016\,s & 71.7 & 0.6\\
\hline
0.50 & 60 & 0.020\,s & 0.006\,s & 3.4 & 1.2\\
 & 100 & 0.068\,s & 0.012\,s & 5.7 & 1.2\\
 & 140 & 0.784\,s & 0.021\,s & 37.7 & 0.8\\
\hline
1.00 & 60 & 0.019\,s & 0.006\,s & 3.5 & 1.6\\
 & 100 & 0.062\,s & 0.012\,s & 5.2 & 1.2\\
 & 140 & 0.600\,s & 0.021\,s & 28.8 & 0.6\\
    \hline
  \end{tabular}
  \caption{Speedup and average number of matches of \fct{glmnet};
    average execution times in seconds.}
  \label{tab:bmlasso}
\end{table}


\section{Conclusions}
\label{sec:conclusions}

An {\R} package for all-subsets variable selection is presented.  It
is based on theoretical strategies that have been recently developed.
A novel algorithm for best-subset variable selection is proposed,
which selects the best variable-subset model according to a
pre-determined search criterion.  It performs considerably faster than
all-subsets variable selection algorithms that rely on the residual
sum of squares only.  Approximation algorithms allow to further
increase the size of tackled datasets.  The package implements {\R}'s
standard formula interface.  A case study is presented, and the
performance of the package is illustrated in a benchmark with various
configurations of simulated datasets.  An extension of the package to
handle missing data merits investigation.


\section*{Acknowledgments}

This work was in part supported by the CRoNoS COST Action (IC1408);
GRUPIN14-005 (Principality of Asturias, Spain); and the
\emph{F\"orderverein des wirtschaftswissenschaftlichen Zentrums der
  Universit\"at Basel} through research project B-123.  The authors
are grateful to Jakob Messner for sharing the GEFS forecast data in
\code{IbkTemperature}.

The authors would particularly like to thank Prof.~Manfred Gilli for
his continued support and encouragements throughout the years.


\bibliography{v92i05}

\end{document}
